{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "id": "ef26921f",
      "cell_type": "markdown",
      "source": "# Q-learning Algorithm\n\nMuhammad Taqui\n01-136221-021",
      "metadata": {}
    },
    {
      "id": "6ca15caf",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Define environment size (11x11 grid)\nenvironment_rows = 11\nenvironment_columns = 11\n\n# Define actions (up, down, left, right)\nactions = ['up', 'right', 'down', 'left']\nnum_actions = len(actions)\n\n# Define rewards (assume -100 for obstacles, -1 for aisles, and 100 for the goal)\nrewards = np.full((environment_rows, environment_columns), -100.)  # Initialize all\nrewards[0, 5] = 100.  # Goal location (item packaging area)\n\n# Define aisles (allowed locations for robots)\naisles = {1: [i for i in range(1, 10)],\n          2: [1, 7, 9],\n          3: [i for i in range(1, 8)] + [9],\n          4: [3, 7],\n          5: [i for i in range(11)],\n          6: [5],\n          7: [i for i in range(1, 10)],\n          8: [3, 7],\n          9: [i for i in range(11)]}\n\n# Set rewards for all aisle locations\nfor row_index in range(1, 10):\n    for column_index in aisles[row_index]:\n        rewards[row_index, column_index] = -1.\n\n# Initialize Q-values (a 3D numpy array of zeros)\nq_values = np.zeros((environment_rows, environment_columns, num_actions))\n\n# Training parameters\nepsilon = 0.9  # Exploration rate\ndiscount_factor = 0.9\nlearning_rate = 0.9\nepisodes = 1000\n\ndef is_terminal_state(row_index, column_index):\n    return rewards[row_index, column_index] == 100.\n\ndef get_next_action(row_index, column_index, epsilon):\n    if random.random() < epsilon:\n        return np.argmax(q_values[row_index, column_index])\n    else:\n        return random.randint(0, num_actions - 1)\n\ndef get_next_location(row_index, column_index, action_index):\n    if actions[action_index] == 'up' and row_index > 0:\n        row_index -= 1\n    elif actions[action_index] == 'down' and row_index < environment_rows - 1:\n        row_index += 1\n    elif actions[action_index] == 'left' and column_index > 0:\n        column_index -= 1\n    elif actions[action_index] == 'right' and column_index < environment_columns - 1:\n        column_index += 1\n    return row_index, column_index\n\ndef run_episode():\n    row_index, column_index = np.random.randint(1, environment_rows), np.random.randint(0, environment_columns)\n    while not is_terminal_state(row_index, column_index):\n        action_index = get_next_action(row_index, column_index, epsilon)\n        old_row_index, old_column_index = row_index, column_index\n        row_index, column_index = get_next_location(row_index, column_index, action_index)\n        reward = rewards[row_index, column_index]\n        old_q_value = q_values[old_row_index, old_column_index, action_index]\n        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n        new_q_value = old_q_value + (learning_rate * temporal_difference)\n        q_values[old_row_index, old_column_index, action_index] = new_q_value\n\nfor episode in range(episodes):\n    run_episode()\n\ndef get_shortest_path(start_row_index, start_column_index):\n    current_row_index, current_column_index = start_row_index, start_column_index\n    path = [(current_row_index, current_column_index)]\n    while not is_terminal_state(current_row_index, current_column_index):\n        action_index = np.argmax(q_values[current_row_index, current_column_index])\n        current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n        path.append((current_row_index, current_column_index))\n    return path\n\ndef plot_path(path, environment_rows, environment_columns):\n    grid = np.zeros((environment_rows, environment_columns))\n    grid[rewards == -100.] = -100\n\n    plt.figure(figsize=(8, 8))\n    plt.imshow(grid, cmap=\"Blues\", origin=\"upper\", extent=[0, environment_columns, 0, environment_rows])\n    for i in range(len(path) - 1):\n        y1, x1 = path[i]\n        y2, x2 = path[i + 1]\n        plt.arrow(x1, y1, x2 - x1, y2 - y1, head_width=0.2, head_length=0.3, fc='red', ec='red')\n    plt.scatter(5, 0, color=\"green\", label=\"Goal\", s=100)\n    plt.scatter(path[0][1], path[0][0], color=\"yellow\", label=\"Start\", s=100)\n    plt.title(\"Shortest Path for Warehouse Robot\")\n    plt.xlabel(\"Columns\")\n    plt.ylabel(\"Rows\")\n    plt.legend()\n    plt.xticks(range(environment_columns))\n    plt.yticks(range(environment_rows))\n    plt.grid(True)\n    plt.gca().invert_yaxis()\n    plt.show()\n\npath_example = get_shortest_path(3, 9)\nplot_path(path_example, environment_rows, environment_columns)\n",
      "outputs": []
    }
  ]
}